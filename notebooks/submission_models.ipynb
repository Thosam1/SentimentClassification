{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T00:14:27.305867Z",
     "start_time": "2025-05-28T00:14:19.496123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import importlib\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")  # Ensure the parent directory is in the path\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from data_preprocessing.dataset_dataloader import create_data_loader\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Local Application/Module Imports ---\n",
    "import data_loader.data_loader\n",
    "import data_preprocessing.data_preprocessing\n",
    "import data_preprocessing.llm_augmentation\n",
    "import models.models\n",
    "import visualizations.visualizations\n",
    "import utils.utils\n",
    "\n",
    "importlib.reload(data_loader.data_loader)\n",
    "from data_loader.data_loader import *\n",
    "\n",
    "importlib.reload(data_preprocessing.data_preprocessing)\n",
    "\n",
    "importlib.reload(data_preprocessing.llm_augmentation)\n",
    "from data_preprocessing.llm_augmentation import *\n",
    "\n",
    "importlib.reload(models.models)\n",
    "from models.models import *\n",
    "\n",
    "importlib.reload(visualizations.visualizations)\n",
    "from visualizations.visualizations import *\n",
    "\n",
    "importlib.reload(utils.utils)\n",
    "from utils.utils import *\n",
    "\n",
    "# --- Notebook Configuration ---\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "# --- Global Settings ---\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ],
   "id": "d8e5a5a8057aca6a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tnorlha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/tnorlha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/tnorlha/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/tnorlha/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tnorlha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/tnorlha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/tnorlha/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/tnorlha/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T00:14:27.333039Z",
     "start_time": "2025-05-28T00:14:27.311093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = get_device()\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "d67d9a7e63135047",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T00:15:26.434082Z",
     "start_time": "2025-05-28T00:15:26.429107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    batch_size: int\n",
    "    model: str\n",
    "    seed: int\n",
    "    lr: float\n",
    "    dropout: float\n",
    "    attention_dropout: float\n",
    "    device: str\n",
    "    num_classes: int\n",
    "    max_len: int\n",
    "\n",
    "config = Config(\n",
    "    batch_size = 16,\n",
    "    model=\"FacebookAI/roberta-large\",\n",
    "    seed=RANDOM_SEED,\n",
    "    lr=3e-5,\n",
    "    dropout=0.1,\n",
    "    attention_dropout=0.1,\n",
    "    device=device,\n",
    "    num_classes=3,\n",
    "    max_len=64,\n",
    ")\n",
    "\n",
    "def load_model(config: Config, model_name, model_path):\n",
    "    config.model = model_name\n",
    "\n",
    "    llm_tokenizer = AutoTokenizer.from_pretrained(config.model)\n",
    "    llm_model = load_blank_model(config)\n",
    "\n",
    "    # Load saved model state dict\n",
    "    state_dict = torch.load(model_path, map_location=device)\n",
    "    llm_model.load_state_dict(state_dict)\n",
    "\n",
    "    llm_model.to(device)\n",
    "    llm_model.eval()\n",
    "\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    return llm_model, llm_tokenizer, loss_fn"
   ],
   "id": "426efe39b8916434",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T00:18:26.573485Z",
     "start_time": "2025-05-28T00:18:26.552991Z"
    }
   },
   "cell_type": "code",
   "source": [
    "submission_df = load_submission_data()\n",
    "\n",
    "def create_submission_data_loader_with_model(llm_tokenizer, config, submission_df):\n",
    "    submission_data_loader = create_data_loader(submission_df, llm_tokenizer, config.max_len, config.batch_size)\n",
    "\n",
    "    return submission_data_loader"
   ],
   "id": "ef2e71976cf4ff31",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T00:20:46.941385Z",
     "start_time": "2025-05-28T00:19:43.887609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"microsoft/deberta-v3-base\"\n",
    "model_path = \"../fine_tuned_models/deberta-v3-base-all.bin\"\n",
    "\n",
    "llm_model, llm_tokenizer, loss_fn = load_model(config, model_name, model_path)\n",
    "\n",
    "submission_data_loader = create_submission_data_loader_with_model(llm_tokenizer, config, submission_df)\n",
    "\n",
    "y_review_texts, y_pred_db, y_pred_probs_db, y_test = get_predictions(\n",
    "    llm_model,\n",
    "    submission_data_loader,\n",
    "    device\n",
    ")"
   ],
   "id": "665f0fba5eb2ad0a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tnorlha/miniconda3/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T00:36:33.632631Z",
     "start_time": "2025-05-28T00:36:33.598229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "label_map = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "generate_submission(y_pred_db, label_map=label_map)"
   ],
   "id": "4819cf128fd8028b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file saved to ../submissions/submission.csv\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5d246de1cbfd20ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d03c46ac7d498186"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T10:02:44.315752Z",
     "start_time": "2025-05-28T10:00:41.728011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"microsoft/deberta-v3-large\"\n",
    "model_path = \"../fine_tuned_models/deberta-v3-large-all.bin\"\n",
    "\n",
    "llm_model, llm_tokenizer, loss_fn = load_model(config, model_name, model_path)\n",
    "\n",
    "submission_data_loader = create_submission_data_loader_with_model(llm_tokenizer, config, submission_df)\n",
    "\n",
    "y_review_texts, y_pred_dl, y_pred_probs_dl, y_test = get_predictions(\n",
    "    llm_model,\n",
    "    submission_data_loader,\n",
    "    device\n",
    ")"
   ],
   "id": "1fc497592723113d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tnorlha/miniconda3/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T10:02:44.358721Z",
     "start_time": "2025-05-28T10:02:44.345777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "label_map = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "generate_submission(y_pred_dl, label_map=label_map)"
   ],
   "id": "f0f34f2626f4d03a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file saved to ../submissions/submission.csv\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "80da137145cc17a4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
